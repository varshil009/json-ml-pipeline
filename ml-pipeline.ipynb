{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11f4d8df-7a7e-4d9b-86e8-e06646bbebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "import json\n",
    "\n",
    "with open(r\"C:\\Users\\91962\\Downloads\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui.json.rtf\", \"r\", encoding=\"utf-8\") as file:\n",
    "    rtf_content = file.read()\n",
    "    \n",
    "text = rtf_to_text(rtf_content)\n",
    "dic = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96f09591-cfc2-4e9e-b762-6761dd18f25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_name': 'test',\n",
       " 'session_description': 'test',\n",
       " 'design_state_data': {'session_info': {'project_id': '1',\n",
       "   'experiment_id': 'kkkk-11',\n",
       "   'dataset': 'iris_modified.csv',\n",
       "   'session_name': 'test',\n",
       "   'session_description': 'test'},\n",
       "  'target': {'prediction_type': 'Regression',\n",
       "   'target': 'petal_width',\n",
       "   'type': 'regression',\n",
       "   'partitioning': True},\n",
       "  'train': {'policy': 'Split the dataset',\n",
       "   'time_variable': 'sepal_length',\n",
       "   'sampling_method': 'No sampling(whole data)',\n",
       "   'split': 'Randomly',\n",
       "   'k_fold': False,\n",
       "   'train_ratio': 0.7,\n",
       "   'random_seed': 0},\n",
       "  'metrics': {'optomize_model_hyperparameters_for': 'AUC',\n",
       "   'optimize_threshold_for': 'F1 Score',\n",
       "   'compute_lift_at': 0,\n",
       "   'cost_matrix_gain_for_true_prediction_true_result': 1,\n",
       "   'cost_matrix_gain_for_true_prediction_false_result': 0,\n",
       "   'cost_matrix_gain_for_false_prediction_true_result': 0,\n",
       "   'cost_matrix_gain_for_false_prediction_false_result': 0},\n",
       "  'feature_handling': {'sepal_length': {'feature_name': 'sepal_length',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'Average of values',\n",
       "     'impute_value': 0}},\n",
       "   'sepal_width': {'feature_name': 'sepal_width',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'custom',\n",
       "     'impute_value': -1}},\n",
       "   'petal_length': {'feature_name': 'petal_length',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'Average of values',\n",
       "     'impute_value': 0}},\n",
       "   'petal_width': {'feature_name': 'petal_width',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'custom',\n",
       "     'impute_value': -2}},\n",
       "   'species': {'feature_name': 'species',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'text',\n",
       "    'feature_details': {'text_handling': 'Tokenize and hash',\n",
       "     'hash_columns': 10}}},\n",
       "  'feature_generation': {'linear_interactions': [['petal_length',\n",
       "     'sepal_width']],\n",
       "   'linear_scalar_type': 'robust',\n",
       "   'polynomial_interactions': ['petal_length/sepal_width',\n",
       "    'petal_width/species'],\n",
       "   'explicit_pairwise_interactions': ['sepal_width/sepal_length',\n",
       "    'petal_width/sepal_length']},\n",
       "  'feature_reduction': {'feature_reduction_method': 'Correlation with target',\n",
       "   'Correlation with target': {'is_selected': True, 'num_features_to_keep': 5},\n",
       "   'Tree-based': {'is_selected': False,\n",
       "    'num_features_to_keep': 0,\n",
       "    'depth_of_trees': 0,\n",
       "    'num_of_trees': 0},\n",
       "   'Principal Component Analysis': {'is_selected': False,\n",
       "    'num_features_to_keep': 0}},\n",
       "  'hyperparameters': {'stratergy': 'Grid Search',\n",
       "   'shuffle_grid': True,\n",
       "   'random_state': 1,\n",
       "   'max_iterations': 2,\n",
       "   'max_search_time': 3,\n",
       "   'parallelism': 5,\n",
       "   'cross_validation_stratergy': 'Time-based K-fold(with overlap)',\n",
       "   'num_of_folds': 6,\n",
       "   'split_ratio': 0,\n",
       "   'stratified': True},\n",
       "  'weighting_stratergy': {'weighting_stratergy_method': 'Sample weights',\n",
       "   'weighting_stratergy_weight_variable': 'petal_length'},\n",
       "  'probability_calibration': {'probability_calibration_method': 'Sigmoid - Platt Scaling'},\n",
       "  'algorithms': {'RandomForestClassifier': {'model_name': 'Random Forest Classifier',\n",
       "    'is_selected': False,\n",
       "    'min_trees': 10,\n",
       "    'max_trees': 30,\n",
       "    'feature_sampling_statergy': 'Default',\n",
       "    'min_depth': 20,\n",
       "    'max_depth': 30,\n",
       "    'min_samples_per_leaf_min_value': 5,\n",
       "    'min_samples_per_leaf_max_value': 50,\n",
       "    'parallelism': 0},\n",
       "   'RandomForestRegressor': {'model_name': 'Random Forest Regressor',\n",
       "    'is_selected': True,\n",
       "    'min_trees': 10,\n",
       "    'max_trees': 20,\n",
       "    'feature_sampling_statergy': 'Default',\n",
       "    'min_depth': 20,\n",
       "    'max_depth': 25,\n",
       "    'min_samples_per_leaf_min_value': 5,\n",
       "    'min_samples_per_leaf_max_value': 10,\n",
       "    'parallelism': 0},\n",
       "   'GBTClassifier': {'model_name': 'Gradient Boosted Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_BoostingStages': [67, 89],\n",
       "    'feature_sampling_statergy': 'Fixed number',\n",
       "    'learningRate': [],\n",
       "    'use_deviance': True,\n",
       "    'use_exponential': False,\n",
       "    'fixed_number': 22,\n",
       "    'min_subsample': 1,\n",
       "    'max_subsample': 2,\n",
       "    'min_stepsize': 0.1,\n",
       "    'max_stepsize': 0.5,\n",
       "    'min_iter': 20,\n",
       "    'max_iter': 40,\n",
       "    'min_depth': 5,\n",
       "    'max_depth': 7},\n",
       "   'GBTRegressor': {'model_name': 'Gradient Boosted Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_BoostingStages': [67, 89],\n",
       "    'feature_sampling_statergy': 'Fixed number',\n",
       "    'use_deviance': True,\n",
       "    'use_exponential': False,\n",
       "    'fixed_number': 22,\n",
       "    'min_subsample': 1,\n",
       "    'max_subsample': 2,\n",
       "    'min_stepsize': 0.1,\n",
       "    'max_stepsize': 0.5,\n",
       "    'min_iter': 20,\n",
       "    'max_iter': 40,\n",
       "    'min_depth': 5,\n",
       "    'max_depth': 7},\n",
       "   'LinearRegression': {'model_name': 'LinearRegression',\n",
       "    'is_selected': False,\n",
       "    'parallelism': 2,\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'LogisticRegression': {'model_name': 'LogisticRegression',\n",
       "    'is_selected': False,\n",
       "    'parallelism': 2,\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'RidgeRegression': {'model_name': 'RidgeRegression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8},\n",
       "   'LassoRegression': {'model_name': 'Lasso Regression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8},\n",
       "   'ElasticNetRegression': {'model_name': 'Lasso Regression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'xg_boost': {'model_name': 'XG Boost',\n",
       "    'is_selected': False,\n",
       "    'use_gradient_boosted_tree': True,\n",
       "    'dart': True,\n",
       "    'tree_method': '',\n",
       "    'random_state': 0,\n",
       "    'max_num_of_trees': 0,\n",
       "    'early_stopping': True,\n",
       "    'early_stopping_rounds': 2,\n",
       "    'max_depth_of_tree': [56, 89],\n",
       "    'learningRate': [89, 76],\n",
       "    'l1_regularization': [77],\n",
       "    'l2_regularization': [78],\n",
       "    'gamma': [68],\n",
       "    'min_child_weight': [67],\n",
       "    'sub_sample': [67],\n",
       "    'col_sample_by_tree': [67],\n",
       "    'replace_missing_values': False,\n",
       "    'parallelism': 0},\n",
       "   'DecisionTreeRegressor': {'model_name': 'Decision Tree',\n",
       "    'is_selected': False,\n",
       "    'min_depth': 4,\n",
       "    'max_depth': 7,\n",
       "    'use_gini': False,\n",
       "    'use_entropy': True,\n",
       "    'min_samples_per_leaf': [12, 6],\n",
       "    'use_best': True,\n",
       "    'use_random': True},\n",
       "   'DecisionTreeClassifier': {'model_name': 'Decision Tree',\n",
       "    'is_selected': False,\n",
       "    'min_depth': 4,\n",
       "    'max_depth': 7,\n",
       "    'use_gini': False,\n",
       "    'use_entropy': True,\n",
       "    'min_samples_per_leaf': [12, 6],\n",
       "    'use_best': True,\n",
       "    'use_random': True},\n",
       "   'SVM': {'model_name': 'Support Vector Machine',\n",
       "    'is_selected': False,\n",
       "    'linear_kernel': True,\n",
       "    'rep_kernel': True,\n",
       "    'polynomial_kernel': True,\n",
       "    'sigmoid_kernel': True,\n",
       "    'c_value': [566, 79],\n",
       "    'auto': True,\n",
       "    'scale': True,\n",
       "    'custom_gamma_values': True,\n",
       "    'tolerance': 7,\n",
       "    'max_iterations': 7},\n",
       "   'SGD': {'model_name': 'Stochastic Gradient Descent',\n",
       "    'is_selected': False,\n",
       "    'use_logistics': True,\n",
       "    'use_modified_hubber_loss': False,\n",
       "    'max_iterations': False,\n",
       "    'tolerance': 56,\n",
       "    'use_l1_regularization': 'on',\n",
       "    'use_l2_regularization': 'on',\n",
       "    'use_elastic_net_regularization': True,\n",
       "    'alpha_value': [79, 56],\n",
       "    'parallelism': 1},\n",
       "   'KNN': {'model_name': 'KNN',\n",
       "    'is_selected': False,\n",
       "    'k_value': [78],\n",
       "    'distance_weighting': True,\n",
       "    'neighbour_finding_algorithm': 'Automatic',\n",
       "    'random_state': 0,\n",
       "    'p_value': 0},\n",
       "   'extra_random_trees': {'model_name': 'Extra Random Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_trees': [45, 489],\n",
       "    'feature_sampling_statergy': 'Square root and Logarithm',\n",
       "    'max_depth': [12, 45],\n",
       "    'min_samples_per_leaf': [78, 56],\n",
       "    'parallelism': 3},\n",
       "   'neural_network': {'model_name': 'Neural Network',\n",
       "    'is_selected': False,\n",
       "    'hidden_layer_sizes': [67, 89],\n",
       "    'activation': '',\n",
       "    'alpha_value': 0,\n",
       "    'max_iterations': 0,\n",
       "    'convergence_tolerance': 0,\n",
       "    'early_stopping': True,\n",
       "    'solver': 'ADAM',\n",
       "    'shuffle_data': True,\n",
       "    'initial_learning_rate': 0,\n",
       "    'automatic_batching': True,\n",
       "    'beta_1': 0,\n",
       "    'beta_2': 0,\n",
       "    'epsilon': 0,\n",
       "    'power_t': 0,\n",
       "    'momentum': 0,\n",
       "    'use_nesterov_momentum': False}}}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cccf66b2-e9cb-40c3-aea4-604a87da5b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['session_name', 'session_description', 'design_state_data'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b32dfcc0-3133-4659-95f9-b8d900abc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1acb88b3-1703-4eef-881f-334cddaa6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_pipeline():\n",
    "    def __init__(self, addrs):\n",
    "        with open(addrs, \"r\", encoding=\"utf-8\") as file:\n",
    "            rtf_content = file.read()\n",
    "        text = rtf_to_text(rtf_content)\n",
    "        self.file = json.loads(text)\n",
    "        self.hashed = []\n",
    "        \n",
    "    def read_df(self):\n",
    "        file = self.file[\"design_state_data\"][\"session_info\"][\"dataset\"]\n",
    "        df = pd.read_csv(file)\n",
    "        print(\"READ THE DATAFRAME\")\n",
    "        return df\n",
    "        \n",
    "    def data_selection(self):\n",
    "        self.df = self.read_df()\n",
    "        # target vars\n",
    "        pred = self.file[\"design_state_data\"][\"target\"][\"prediction_type\"]\n",
    "        self.target = self.file[\"design_state_data\"][\"target\"][\"target\"]\n",
    "        type_ = self.file[\"design_state_data\"][\"target\"][\"type\"]\n",
    "        partition = self.file[\"design_state_data\"][\"target\"][\"partitioning\"]\n",
    "\n",
    "        # train vars\n",
    "        train_columns = [x for x in self.df.columns if x != self.target]\n",
    "        train_size = self.file[\"design_state_data\"][\"train\"][\"train_ratio\"]\n",
    "        \n",
    "        policy = self.file[\"design_state_data\"][\"train\"][\"policy\"]\n",
    "        split1 = \"random\" in self.file[\"design_state_data\"][\"train\"][\"policy\"].lower()\n",
    "        split2 = \"strat\" in self.file[\"design_state_data\"][\"train\"][\"policy\"].lower()\n",
    "        randomness = self.file[\"design_state_data\"][\"train\"][\"random_seed\"]\n",
    "        KFOLD = self.file[\"design_state_data\"][\"train\"][\"k_fold\"]\n",
    "        \n",
    "        # train test split\n",
    "        if \"split\" in policy.lower():\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                                                self.df[train_columns], \n",
    "                                                                self.df[self.target], \n",
    "                                                                train_size = train_size, \n",
    "                                                                shuffle = split1,\n",
    "                                                                #stratify = split2,\n",
    "                                                                random_state = randomness \n",
    "                                                                )\n",
    "            \n",
    "            X_train, X_test, Y_train, Y_test = self.feature_handling(X_train, X_test, Y_train, Y_test)\n",
    "            X_train, X_test = self.feature_generation(X_train, X_test)\n",
    "            print(\"SPLIT IMPUTATION COMPLETE\")\n",
    "            return \"normal_split\", [X_train, X_test, Y_train, Y_test]\n",
    "            \n",
    "        if KFOLD:\n",
    "            kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "            folds = []\n",
    "            # Generate splits\n",
    "            for train_idx, test_idx in kf.split(df):\n",
    "                X_train = df.loc[train_idx, train_columns]\n",
    "                X_test = df.loc[test_idx, train_columns]\n",
    "                Y_train = df.loc[train_idx, target]\n",
    "                Y_test = df.loc[test_idx, target]\n",
    "                \n",
    "                X_train, X_test, Y_train, Y_test = self.feature_handling(X_train, X_test, Y_train, Y_test)\n",
    "                X_train, X_test = self.feature_generation(X_train, X_test)\n",
    "                folds.append([X_train, X_test, Y_train, Y_test])\n",
    "                \n",
    "            print(\"KFOLDED IMPUTATION COMPLETE\")\n",
    "            return \"kfold\", folds\n",
    "    \n",
    "    def feature_handling(self, X_train, X_test, Y_train, Y_test):\n",
    "        # list features\n",
    "        features = list(self.file[\"design_state_data\"][\"feature_handling\"].keys())\n",
    "        # grab the feature dictionary part\n",
    "        features_dic = self.file[\"design_state_data\"][\"feature_handling\"]\n",
    "        #print(features_dic)\n",
    "        for feature in features:\n",
    "            if features_dic[feature][\"is_selected\"]:\n",
    "                if features_dic[feature][\"feature_variable_type\"].lower() == \"numerical\":\n",
    "                    imp_method = features_dic[feature][\"feature_details\"][\"impute_with\"].lower()\n",
    "                    if imp_method == \"custom\":\n",
    "                        imp_value = features_dic[feature][\"feature_details\"][\"impute_value\"]\n",
    "                        if feature == self.target:\n",
    "                            Y_train.fillna(imp_value,  inplace = True)\n",
    "                            Y_test.fillna(imp_value,  inplace = True) \n",
    "                        else:\n",
    "                            X_train.fillna({feature : imp_value}, inplace = True)\n",
    "                            X_test.fillna({feature : imp_value},  inplace = True)\n",
    "    \n",
    "                    else:\n",
    "                        if features_dic[feature][\"feature_variable_type\"].lower() == \"numerical\":\n",
    "                            if \"mean\" in imp_method or \"average\" in imp_method:\n",
    "                                strategy = \"mean\"\n",
    "                            elif \"median\" in imp_method:\n",
    "                                strategy = \"median\"\n",
    "                            elif \"mode\" in imp_method:\n",
    "                                strategy = \"mode\"\n",
    "                            si = SimpleImputer(strategy = strategy)\n",
    "    \n",
    "                            if feature == self.target:\n",
    "                                si.fit(Y_train)\n",
    "                                Y_train = si.transform(Y_train)\n",
    "                                Y_test = si.transform(Y_test)\n",
    "                            else:\n",
    "                                si.fit(X_train[feature].values.reshape(-1, 1))\n",
    "                                X_train[feature] = si.transform(X_train[feature].values.reshape(-1, 1)).ravel()\n",
    "                                X_test[feature] = si.transform(X_test[feature].values.reshape(-1, 1)).ravel()\n",
    "                                \n",
    "                else: # for text data\n",
    "                    self.hashed.append(feature)\n",
    "                    strategy = \"most_frequent\"\n",
    "    \n",
    "                    si = SimpleImputer(strategy = strategy)\n",
    "    \n",
    "                    if feature == self.target:\n",
    "                        si.fit(Y_train.values.reshape(-1, 1))\n",
    "                        Y_train = si.transform(Y_train.values.reshape(-1, 1)).ravel()\n",
    "                        Y_test = si.transform(Y_test.values.reshape(-1, 1)).ravel()\n",
    "                    else:\n",
    "                        si.fit(X_train[feature].values.reshape(-1, 1))\n",
    "                        X_train[feature] = si.transform(X_train[feature].values.reshape(-1, 1)).ravel()\n",
    "                        X_test[feature] = si.transform(X_test[feature].values.reshape(-1, 1)).ravel()\n",
    "                    \"\"\"\n",
    "                    not expecting the target variable to be hashed or tokenized so ignoring y_train, \n",
    "                    but if it is, we will use Label encoder instead\n",
    "                    \"\"\"\n",
    "                    if feature == self.target:\n",
    "                        e = LabelEncoder()\n",
    "                        e.fit(Y_train)\n",
    "                        Y_train = e.transform(Y_train)\n",
    "                        Y_test = e.transform(Y_test)\n",
    "                        \n",
    "                    else:\n",
    "                        # use hashing evctorizer\n",
    "                        hash_columns = features_dic[feature][\"feature_details\"][\"hash_columns\"]\n",
    "                        if hash_columns != 0: \n",
    "                            vectorizer = HashingVectorizer(n_features = hash_columns, stop_words = 'english', alternate_sign = False)\n",
    "            \n",
    "                            hashed_X_train = vectorizer.transform(X_train[feature].astype(str)).toarray()\n",
    "                            hashed_X_test = vectorizer.transform(X_test[feature].astype(str)).toarray()\n",
    "                            \n",
    "                            # vectorizer returns dense array create columns out of that \n",
    "                            hashed_X_train_df = pd.DataFrame(hashed_X_train, columns=[f\"hashed_{feature}_{i}\" for i in range(hashed_X_train.shape[1])])\n",
    "                            hashed_X_test_df = pd.DataFrame(hashed_X_test, columns=[f\"hashed_{feature}_{i}\" for i in range(hashed_X_test.shape[1])])\n",
    "                            # join\n",
    "                            X_train = pd.concat([X_train, hashed_X_train_df], axis=1)\n",
    "                            X_test = pd.concat([X_test, hashed_X_test_df], axis=1)\n",
    "\n",
    "                            # drop the string col\n",
    "                            X_train.drop(feature, axis = 1, inplace = True)\n",
    "                            X_test.drop(feature, axis = 1, inplace = True)\n",
    "                            \n",
    "                        else:\n",
    "                            e = LabelEncoder()\n",
    "                            e.fit(X_train[feature])\n",
    "                            X_train[feature] = e.transform(X_train[feature].values.reshape(-1, 1)).flatten()\n",
    "                            X_test[feature]= e.transform(X_test[feature].values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        return X_train, X_test, Y_train, Y_test  \n",
    "\n",
    "    def interaction(self, interaction_list, interaction_type, X_train, X_test):\n",
    "        \"\"\"\n",
    "        handles interaction and takes care of hashed array interactions if any \n",
    "        \"\"\"\n",
    "        functiondic = {\n",
    "                       \"linear\" : lambda x, y : x + y,\n",
    "                       \"poly\" : lambda x, y : x / y, \n",
    "                       \"expl\" : lambda x, y : x / y\n",
    "                      }\n",
    "        \n",
    "        function = functiondic[interaction_type]\n",
    "        for x, y in interaction_list:\n",
    "            # skip when target is given for interaction, this causes dataleakage\n",
    "            if y == self.target: continue\n",
    "            if x == self.target: continue\n",
    "            \n",
    "            if x in self.hashed:\n",
    "                cols = [p for p in X_train.columns if f\"hashed_{x}\" in p]\n",
    "                for hashed_col in cols:\n",
    "                    X_train[f\"{hashed_col}_l_{y}\"] = function(X_train[hashed_col], X_train[y])\n",
    "                    X_test[f\"{hashed_col}_l_{y}\"] = function(X_test[hashed_col], X_test[y])\n",
    "\n",
    "            elif y in self.hashed:\n",
    "                cols = [p for p in X_train.columns if f\"hashed_{y}\" in p]\n",
    "                for hashed_col in cols:\n",
    "                    X_train[f\"{x}_l_{hashed_col}\"] = function(X_train[x], X_train[hashed_col])\n",
    "                    X_test[f\"{x}_l_{hashed_col}\"] = function(X_test[x], X_test[hashed_col])\n",
    "            \n",
    "            # not expecting both to be hashed would be worst case scenario\n",
    "            elif x in self.hashed and y in self.hashed:\n",
    "                colx = [p for p in X_train.columns if f\"hashed_{x}\" in p]\n",
    "                coly = [p for p in X_train.columns if f\"hashed_{y}\" in p]\n",
    "                \n",
    "                for hashed_x in colx:\n",
    "                    for hashed_y in coly:\n",
    "                        X_train[f\"{hashed_x}_l_{hashed_y}\"] = function(X_train[hashed_x], X_train[hashed_y])\n",
    "                        X_test[f\"{hashed_x}_l_{hashed_y}\"] = function(X_test[hashed_x], X_test[hashed_y])\n",
    "            else:\n",
    "                X_train[f\"{x}_l_{y}\"] = function(X_train[x], X_train[y])\n",
    "                X_test[f\"{x}_l_{y}\"] = function(X_test[x], X_test[y])\n",
    "                \n",
    "        return X_train, X_test\n",
    "        \n",
    "\n",
    "    def feature_generation(self, X_train, X_test):\n",
    "        dic = self.file[\"design_state_data\"][\"feature_generation\"]\n",
    "        \n",
    "        # add new linear features\n",
    "        X_train, X_test = self.interaction(dic[\"linear_interactions\"], \"linear\", X_train, X_test)\n",
    "            \n",
    "        # add polynomial features\n",
    "        polynomial_interactions = [x.split(\"/\") for x in dic[\"polynomial_interactions\"]]\n",
    "        X_train, X_test = self.interaction(polynomial_interactions, \"poly\", X_train, X_test)\n",
    "        \n",
    "        # add explicit pariwaise reln\n",
    "        explicit_pairwise_interactions = [x.split(\"/\") for x in dic[\"explicit_pairwise_interactions\"]]\n",
    "        X_train, X_test = self.interaction(explicit_pairwise_interactions, \"expl\", X_train, X_test)\n",
    "\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb7f9787-a7a3-44cd-9585-c806463996e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ THE DATAFRAME\n",
      "SPLIT IMPUTATION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "k = preprocess_pipeline(r\"C:\\Users\\91962\\Downloads\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui.json.rtf\")\n",
    "u = k.data_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d41a8206-9f4a-4c2e-a15d-f72ddcaf48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_reduction_pipeline:\n",
    "    def __init__(self, dic, X_train, X_test, target):\n",
    "        self.dic = dic\n",
    "        self.method = dic[\"feature_reduction_method\"].lower()\n",
    "        self.itrain = X_train.index\n",
    "        self.itest = X_test.index\n",
    "        self.df = pd.concat([X_train, X_test], axis = 0)\n",
    "        self.target = target\n",
    "    \n",
    "    def execute(self):\n",
    "        if \"correlation\" in self.method:\n",
    "            return self.correlation(0.4)\n",
    "            \n",
    "        elif \"principal\" in self.method:\n",
    "            return self.pca(self.dic[self.method][\"num_features_to_keep\"])\n",
    "            \n",
    "        elif \"tree\" in self.method:\n",
    "            depth = self.dic[self.method][\"depth_of_trees\"]\n",
    "            numt = self.dic[self.method][\"num_of_trees\"]\n",
    "            n = self.dic[self.method][\"num_features_to_keep\"]\n",
    "            return self.tree(self.df, num_features_to_keep = n, num_trees = numt, depth = depth)\n",
    "        \n",
    "        else:\n",
    "            return self.df.iloc[self.itrain], self.df.iloc[self.itest]\n",
    "            \n",
    "    def correlation(self, threshold = 0.4):\n",
    "        corr_matrix = self.df.corr().abs()\n",
    "        # upper triangle of correlation matrix\n",
    "         # Keeps the values where the condition is True and replaces everything else with NaN.                  \n",
    "        \"\"\"                 |    # makes it triangle # creates a corelation matrix like array with ones\n",
    "                            |           |                |\n",
    "                            v           v                v                 \"\"\"\n",
    "        \"\"\"\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if upper.iloc[i, j] >= threshold:\n",
    "                    to_drop.append(corr_matrix.columns[i])\n",
    "        keep = [column for column in corr_matrix.columns if column not in to_drop]\n",
    "        return self.df.loc[self.itrain, keep], self.df.loc[self.itest, keep]\n",
    "        \"\"\"\n",
    "\n",
    "        corr_matrix = self.df.corr().abs()\n",
    "        # Upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = []\n",
    "        for column in upper.columns:\n",
    "            # Find columns where correlation exceeds threshold\n",
    "            if any(upper[column] >= threshold):\n",
    "                to_drop.append(column)\n",
    "                \n",
    "        keep = [column for column in corr_matrix.columns if column not in to_drop]\n",
    "        return self.df.loc[self.itrain, keep], self.df.loc[self.itest, keep]\n",
    "\n",
    "    def pca(self, n = 3):\n",
    "        pca = PCA(n_components=num_features_to_keep)\n",
    "        df_pca = pca.fit_transform(self.df)\n",
    "        return df_pca.iloc[self.itrain], df_pca.iloc[self.itest]\n",
    "\n",
    "    def tree(df, num_features_to_keep = 10, num_trees = 100, depth = 6):\n",
    "        # fit an rf model\n",
    "        rf = RandomForestClassifier(n_estimators = num_trees, max_depth = depth)\n",
    "        y = self.df[self.target]\n",
    "        X = self.df[[x for x in self.df.columns if x != self.target]]\n",
    "        rf.fit(X, y)\n",
    "        # select top features using feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = X.columns\n",
    "    \n",
    "        # sort by importance and select top N\n",
    "        sorted_indices = importances.argsort()[::-1][:num_features_to_keep]\n",
    "        selected_features = feature_names[sorted_indices]\n",
    "    \n",
    "        # subset the data\n",
    "        X_train_reduced = self.df.loc[self.itrain, selected_features]\n",
    "        X_test_reduced = self.df.loc[self.itest, selected_features]\n",
    "    \n",
    "        return X_train_reduced, X_test_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5cc6a32c-cf6b-42b2-8954-03ff00711b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_reduction_pipeline:\n",
    "    def __init__(self, dic, X_train, X_test, target):\n",
    "        self.dic = dic\n",
    "        self.method = dic[\"feature_reduction_method\"].lower()\n",
    "        self.X_train = X_train.copy()\n",
    "        self.X_test = X_test.copy()\n",
    "        self.target = target\n",
    "    \n",
    "    def execute(self):\n",
    "        if \"correlation\" in self.method:\n",
    "            return self.correlation(0.4)\n",
    "            \n",
    "        elif \"principal\" in self.method:\n",
    "            return self.pca(self.dic[self.method][\"num_features_to_keep\"])\n",
    "            \n",
    "        elif \"tree\" in self.method:\n",
    "            depth = self.dic[self.method][\"depth_of_trees\"]\n",
    "            numt = self.dic[self.method][\"num_of_trees\"]\n",
    "            n = self.dic[self.method][\"num_features_to_keep\"]\n",
    "            return self.tree(num_features_to_keep=n, num_trees=numt, depth=depth)\n",
    "        \n",
    "        else:\n",
    "            return self.X_train, self.X_test\n",
    "            \n",
    "    def correlation(self, threshold=0.4):\n",
    "        combined = pd.concat([self.X_train, self.X_test])\n",
    "        corr_matrix = combined.corr().abs()\n",
    "        \n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = []\n",
    "        \n",
    "        for column in upper.columns:\n",
    "            if any(upper[column] >= threshold):\n",
    "                to_drop.append(column)\n",
    "                \n",
    "        keep = [column for column in corr_matrix.columns if column not in to_drop]\n",
    "        \n",
    "        return self.X_train[keep], self.X_test[keep]\n",
    "    \n",
    "    def pca(self, n=3):\n",
    "        combined = pd.concat([self.X_train, self.X_test])\n",
    "        \n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(combined)\n",
    "        \n",
    "        train_transformed = pca.transform(self.X_train)\n",
    "        test_transformed = pca.transform(self.X_test)\n",
    "        \n",
    "        cols = [f'PC{i+1}' for i in range(n)]\n",
    "        train_pca = pd.DataFrame(train_transformed, index = self.X_train.index, columns = cols)\n",
    "        test_pca = pd.DataFrame(test_transformed, index = self.X_test.index, columns = cols)\n",
    "        \n",
    "        return train_pca, test_pca\n",
    "    \n",
    "    def tree(self, num_features_to_keep=10, num_trees=100, depth=6):\n",
    "        if self.target in self.X_train.columns:\n",
    "            X = self.X_train.drop(columns=[self.target])\n",
    "            y = self.X_train[self.target]\n",
    "        else:\n",
    "            X = self.X_train\n",
    "            raise ValueError(\"Target column not found in X_train. Please ensure target is available.\")\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators = num_trees, max_depth = depth)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Sort features by importance\n",
    "        features_with_importance = list(zip(feature_names, importances))\n",
    "        sorted_features = sorted(features_with_importance, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Select top N features\n",
    "        selected_features = [f[0] for f in sorted_features[:num_features_to_keep]]\n",
    "        \n",
    "        # Subset the data - ensuring we only use available columns\n",
    "        X_train_reduced = self.X_train[selected_features]\n",
    "        X_test_reduced = self.X_test[selected_features]\n",
    "        \n",
    "        return X_train_reduced, X_test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9da9ba2e-cb84-406b-b701-c60426fb7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dic = dic[\"design_state_data\"][\"feature_reduction\"]\n",
    "target = dic[\"design_state_data\"][\"target\"][\"target\"]\n",
    "dimred = feature_reduction_pipeline(local_dic, u[1][0], u[1][1], target)\n",
    "k = dimred.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e9ac08d-8a97-487b-a11f-4892b69eb458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>hashed_species_0</th>\n",
       "      <th>hashed_species_2</th>\n",
       "      <th>hashed_species_3</th>\n",
       "      <th>hashed_species_4</th>\n",
       "      <th>hashed_species_5</th>\n",
       "      <th>hashed_species_6</th>\n",
       "      <th>hashed_species_8</th>\n",
       "      <th>hashed_species_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  hashed_species_0  hashed_species_2  \\\n",
       "0             5.1          3.5               0.0               0.0   \n",
       "1             4.9          3.0               0.0               0.0   \n",
       "2             4.7          3.2               0.0               0.0   \n",
       "3             4.6          3.1               0.0               0.0   \n",
       "4             5.0          3.6               0.0               0.0   \n",
       "..            ...          ...               ...               ...   \n",
       "100           6.3          3.3               0.0               0.0   \n",
       "101           5.8          2.7               0.0               0.0   \n",
       "102           7.1          3.0               0.0               0.0   \n",
       "103           6.3          2.9               0.0               0.0   \n",
       "104           6.5          3.0               0.0               0.0   \n",
       "\n",
       "     hashed_species_3  hashed_species_4  hashed_species_5  hashed_species_6  \\\n",
       "0                 0.0               0.0               0.0          0.707107   \n",
       "1                 0.0               0.0               0.0          0.707107   \n",
       "2                 0.0               0.0               0.0          0.707107   \n",
       "3                 0.0               0.0               0.0          0.707107   \n",
       "4                 0.0               0.0               0.0          0.707107   \n",
       "..                ...               ...               ...               ...   \n",
       "100               0.0               0.0               0.0          0.707107   \n",
       "101               0.0               0.0               0.0          0.707107   \n",
       "102               0.0               0.0               0.0          0.707107   \n",
       "103               0.0               0.0               0.0          0.707107   \n",
       "104               0.0               0.0               0.0          0.707107   \n",
       "\n",
       "     hashed_species_8  hashed_species_9  \n",
       "0                 0.0               0.0  \n",
       "1                 0.0               0.0  \n",
       "2                 0.0               0.0  \n",
       "3                 0.0               0.0  \n",
       "4                 0.0               0.0  \n",
       "..                ...               ...  \n",
       "100               0.0               0.0  \n",
       "101               0.0               0.0  \n",
       "102               0.0               0.0  \n",
       "103               0.0               0.0  \n",
       "104               0.0               0.0  \n",
       "\n",
       "[105 rows x 10 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a2292-42f2-430a-b9e1-36bcc9c858df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-cuda",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
